{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features_properties_id</th>\n",
       "      <th>features_properties_title_en</th>\n",
       "      <th>features_properties_description_en</th>\n",
       "      <th>features_properties_keywords_en</th>\n",
       "      <th>metadata_en</th>\n",
       "      <th>metadata_en_processed</th>\n",
       "      <th>metadata_en_preprocessed_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000183ed-8864-42f0-ae43-c4313a860720</td>\n",
       "      <td>Principal Mineral Areas, Producing Mines, and ...</td>\n",
       "      <td>This dataset is produced and published annuall...</td>\n",
       "      <td>mineralization, mineral occurrences, mines, hy...</td>\n",
       "      <td>Principal Mineral Areas, Producing Mines, and ...</td>\n",
       "      <td>principal mineral areas, producing mines, oil ...</td>\n",
       "      <td>principal mineral areas producing mines oil ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7f245e4d-76c2-4caa-951a-45d1d2051333</td>\n",
       "      <td>Canadian Digital Elevation Model, 1945-2011</td>\n",
       "      <td>This collection is a legacy product that is no...</td>\n",
       "      <td>Canada, Earth Sciences, elevation, relief, geo...</td>\n",
       "      <td>Canadian Digital Elevation Model, 1945-2011 Th...</td>\n",
       "      <td>canadian digital elevation model, 1945-2011 co...</td>\n",
       "      <td>canadian digital elevation model collection le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>085024ac-5a48-427a-a2ea-d62af73f2142</td>\n",
       "      <td>Canada's National Earthquake Scenario Catalogue</td>\n",
       "      <td>The National Earthquake Scenario Catalogue, pr...</td>\n",
       "      <td>Emergency preparedness, Earth sciences, Earthq...</td>\n",
       "      <td>Canada's National Earthquake Scenario Catalogu...</td>\n",
       "      <td>canada's national earthquake scenario catalogu...</td>\n",
       "      <td>canada national earthquake scenario catalogue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03ccfb5c-a06e-43e3-80fd-09d4f8f69703</td>\n",
       "      <td>Temporal Series of the National Air Photo Libr...</td>\n",
       "      <td>Note: To visualize the data in the viewer, zoo...</td>\n",
       "      <td>Mosaic, Aerial photography, Access to informat...</td>\n",
       "      <td>Temporal Series of the National Air Photo Libr...</td>\n",
       "      <td>temporal series national air photo library (na...</td>\n",
       "      <td>temporal series national air photo library nap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>488faf70-b50b-4749-ac1c-a1fd44e06f11</td>\n",
       "      <td>Indigenous Mining Agreements</td>\n",
       "      <td>The Indigenous Mining Agreements dataset provi...</td>\n",
       "      <td>Indigenous, First Nations, Métis, Indigenous a...</td>\n",
       "      <td>Indigenous Mining Agreements The Indigenous Mi...</td>\n",
       "      <td>indigenous mining agreements indigenous mining...</td>\n",
       "      <td>indigenous mining agreements indigenous mining...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features_properties_id  \\\n",
       "0  000183ed-8864-42f0-ae43-c4313a860720   \n",
       "1  7f245e4d-76c2-4caa-951a-45d1d2051333   \n",
       "2  085024ac-5a48-427a-a2ea-d62af73f2142   \n",
       "3  03ccfb5c-a06e-43e3-80fd-09d4f8f69703   \n",
       "4  488faf70-b50b-4749-ac1c-a1fd44e06f11   \n",
       "\n",
       "                        features_properties_title_en  \\\n",
       "0  Principal Mineral Areas, Producing Mines, and ...   \n",
       "1        Canadian Digital Elevation Model, 1945-2011   \n",
       "2    Canada's National Earthquake Scenario Catalogue   \n",
       "3  Temporal Series of the National Air Photo Libr...   \n",
       "4                       Indigenous Mining Agreements   \n",
       "\n",
       "                  features_properties_description_en  \\\n",
       "0  This dataset is produced and published annuall...   \n",
       "1  This collection is a legacy product that is no...   \n",
       "2  The National Earthquake Scenario Catalogue, pr...   \n",
       "3  Note: To visualize the data in the viewer, zoo...   \n",
       "4  The Indigenous Mining Agreements dataset provi...   \n",
       "\n",
       "                     features_properties_keywords_en  \\\n",
       "0  mineralization, mineral occurrences, mines, hy...   \n",
       "1  Canada, Earth Sciences, elevation, relief, geo...   \n",
       "2  Emergency preparedness, Earth sciences, Earthq...   \n",
       "3  Mosaic, Aerial photography, Access to informat...   \n",
       "4  Indigenous, First Nations, Métis, Indigenous a...   \n",
       "\n",
       "                                         metadata_en  \\\n",
       "0  Principal Mineral Areas, Producing Mines, and ...   \n",
       "1  Canadian Digital Elevation Model, 1945-2011 Th...   \n",
       "2  Canada's National Earthquake Scenario Catalogu...   \n",
       "3  Temporal Series of the National Air Photo Libr...   \n",
       "4  Indigenous Mining Agreements The Indigenous Mi...   \n",
       "\n",
       "                               metadata_en_processed  \\\n",
       "0  principal mineral areas, producing mines, oil ...   \n",
       "1  canadian digital elevation model, 1945-2011 co...   \n",
       "2  canada's national earthquake scenario catalogu...   \n",
       "3  temporal series national air photo library (na...   \n",
       "4  indigenous mining agreements indigenous mining...   \n",
       "\n",
       "                      metadata_en_preprocessed_token  \n",
       "0  principal mineral areas producing mines oil ga...  \n",
       "1  canadian digital elevation model collection le...  \n",
       "2  canada national earthquake scenario catalogue ...  \n",
       "3  temporal series national air photo library nap...  \n",
       "4  indigenous mining agreements indigenous mining...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_parquet(\"../data/Processed_records.parquet\")\n",
    "data = data.dropna()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features_properties_id</th>\n",
       "      <th>features_properties_title_en</th>\n",
       "      <th>metadata_en_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000183ed-8864-42f0-ae43-c4313a860720</td>\n",
       "      <td>Principal Mineral Areas, Producing Mines, and ...</td>\n",
       "      <td>principal mineral areas, producing mines, oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7f245e4d-76c2-4caa-951a-45d1d2051333</td>\n",
       "      <td>Canadian Digital Elevation Model, 1945-2011</td>\n",
       "      <td>canadian digital elevation model, 1945-2011 co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>085024ac-5a48-427a-a2ea-d62af73f2142</td>\n",
       "      <td>Canada's National Earthquake Scenario Catalogue</td>\n",
       "      <td>canada's national earthquake scenario catalogu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03ccfb5c-a06e-43e3-80fd-09d4f8f69703</td>\n",
       "      <td>Temporal Series of the National Air Photo Libr...</td>\n",
       "      <td>temporal series national air photo library (na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>488faf70-b50b-4749-ac1c-a1fd44e06f11</td>\n",
       "      <td>Indigenous Mining Agreements</td>\n",
       "      <td>indigenous mining agreements indigenous mining...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features_properties_id  \\\n",
       "0  000183ed-8864-42f0-ae43-c4313a860720   \n",
       "1  7f245e4d-76c2-4caa-951a-45d1d2051333   \n",
       "2  085024ac-5a48-427a-a2ea-d62af73f2142   \n",
       "3  03ccfb5c-a06e-43e3-80fd-09d4f8f69703   \n",
       "4  488faf70-b50b-4749-ac1c-a1fd44e06f11   \n",
       "\n",
       "                        features_properties_title_en  \\\n",
       "0  Principal Mineral Areas, Producing Mines, and ...   \n",
       "1        Canadian Digital Elevation Model, 1945-2011   \n",
       "2    Canada's National Earthquake Scenario Catalogue   \n",
       "3  Temporal Series of the National Air Photo Libr...   \n",
       "4                       Indigenous Mining Agreements   \n",
       "\n",
       "                               metadata_en_processed  \n",
       "0  principal mineral areas, producing mines, oil ...  \n",
       "1  canadian digital elevation model, 1945-2011 co...  \n",
       "2  canada's national earthquake scenario catalogu...  \n",
       "3  temporal series national air photo library (na...  \n",
       "4  indigenous mining agreements indigenous mining...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df = pd.read_csv(\"df_training_full.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into training and test sets.\n",
    "train_set = df.sample(frac=0.9, random_state=42)  # Fixing the seed to 42 to reproducibility.\n",
    "test_set = df.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from simpletransformers. To fine-tune a language model, each sample should be a row in a text file.\n",
    "# Store the 'metadata_en_processed' column in a text file.\n",
    "with open('../data/simpletransformer_lm_train.txt', 'w') as f:\n",
    "    for item in train_set['metadata_en_processed']:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Store the test set in a text file.\n",
    "with open('../data/simpletransformer_lm_test.txt', 'w') as f:\n",
    "    for item in test_set['metadata_en_processed']:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpletransformers code for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.dataset_type = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"adafactor_beta1\": null,\n",
      "  \"adafactor_clip_threshold\": 1.0,\n",
      "  \"adafactor_decay_rate\": -0.8,\n",
      "  \"adafactor_eps\": [\n",
      "    1e-30,\n",
      "    0.001\n",
      "  ],\n",
      "  \"adafactor_relative_step\": true,\n",
      "  \"adafactor_scale_parameter\": true,\n",
      "  \"adafactor_warmup_init\": true,\n",
      "  \"adam_betas\": [\n",
      "    0.9,\n",
      "    0.999\n",
      "  ],\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"best_model_dir\": \"outputs/best_model\",\n",
      "  \"cache_dir\": \"cache_dir/\",\n",
      "  \"config\": {},\n",
      "  \"cosine_schedule_num_cycles\": 0.5,\n",
      "  \"custom_layer_parameters\": [],\n",
      "  \"custom_parameter_groups\": [],\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"do_lower_case\": false,\n",
      "  \"dynamic_quantize\": false,\n",
      "  \"early_stopping_consider_epochs\": false,\n",
      "  \"early_stopping_delta\": 0,\n",
      "  \"early_stopping_metric\": \"eval_loss\",\n",
      "  \"early_stopping_metric_minimize\": true,\n",
      "  \"early_stopping_patience\": 3,\n",
      "  \"encoding\": null,\n",
      "  \"eval_batch_size\": 8,\n",
      "  \"evaluate_during_training\": false,\n",
      "  \"evaluate_during_training_silent\": true,\n",
      "  \"evaluate_during_training_steps\": 2000,\n",
      "  \"evaluate_during_training_verbose\": false,\n",
      "  \"evaluate_each_epoch\": true,\n",
      "  \"fp16\": true,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"learning_rate\": 4e-05,\n",
      "  \"local_rank\": -1,\n",
      "  \"logging_steps\": 50,\n",
      "  \"loss_type\": null,\n",
      "  \"loss_args\": {},\n",
      "  \"manual_seed\": null,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"max_seq_length\": 128,\n",
      "  \"model_name\": null,\n",
      "  \"model_type\": null,\n",
      "  \"multiprocessing_chunksize\": -1,\n",
      "  \"n_gpu\": 1,\n",
      "  \"no_cache\": false,\n",
      "  \"no_save\": false,\n",
      "  \"not_saved_args\": [],\n",
      "  \"num_train_epochs\": 1,\n",
      "  \"optimizer\": \"AdamW\",\n",
      "  \"output_dir\": \"outputs/\",\n",
      "  \"overwrite_output_dir\": true,\n",
      "  \"polynomial_decay_schedule_lr_end\": 1e-07,\n",
      "  \"polynomial_decay_schedule_power\": 1.0,\n",
      "  \"process_count\": 6,\n",
      "  \"quantized_model\": false,\n",
      "  \"reprocess_input_data\": true,\n",
      "  \"save_best_model\": true,\n",
      "  \"save_eval_checkpoints\": true,\n",
      "  \"save_model_every_epoch\": true,\n",
      "  \"save_optimizer_and_scheduler\": true,\n",
      "  \"save_steps\": 2000,\n",
      "  \"scheduler\": \"linear_schedule_with_warmup\",\n",
      "  \"silent\": false,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"tensorboard_dir\": null,\n",
      "  \"thread_count\": null,\n",
      "  \"tokenizer_name\": null,\n",
      "  \"tokenizer_type\": null,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"train_custom_parameters_only\": false,\n",
      "  \"use_cached_eval_features\": false,\n",
      "  \"use_early_stopping\": false,\n",
      "  \"use_hf_datasets\": false,\n",
      "  \"use_multiprocessing\": true,\n",
      "  \"use_multiprocessing_for_evaluation\": true,\n",
      "  \"wandb_kwargs\": {},\n",
      "  \"wandb_project\": null,\n",
      "  \"warmup_ratio\": 0.06,\n",
      "  \"warmup_steps\": 0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"model_class\": \"LanguageModelingModel\",\n",
      "  \"block_size\": -1,\n",
      "  \"config_name\": null,\n",
      "  \"dataset_class\": null,\n",
      "  \"dataset_type\": \"simple\",\n",
      "  \"discriminator_config\": {},\n",
      "  \"discriminator_loss_weight\": 50.0,\n",
      "  \"generator_config\": {},\n",
      "  \"max_steps\": -1,\n",
      "  \"min_frequency\": 2,\n",
      "  \"mlm\": true,\n",
      "  \"mlm_probability\": 0.15,\n",
      "  \"sliding_window\": false,\n",
      "  \"special_tokens\": [\n",
      "    \"<s>\",\n",
      "    \"<pad>\",\n",
      "    \"</s>\",\n",
      "    \"<unk>\",\n",
      "    \"<mask>\"\n",
      "  ],\n",
      "  \"stride\": 0.8,\n",
      "  \"tie_generator_and_discriminator_embeddings\": true,\n",
      "  \"vocab_size\": null,\n",
      "  \"clean_text\": true,\n",
      "  \"handle_chinese_chars\": true,\n",
      "  \"special_tokens_list\": [],\n",
      "  \"strip_accents\": true\n",
      "}\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# Print the model arguments.\n",
    "print(json.dumps(model_args.__dict__, indent=2))\n",
    "print(len(model_args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the language modelling train file.\n",
    "train_file = \"../data/simpletransformer_lm_train.txt\"\n",
    "test_file = \"../data/simpletransformer_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.75MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 36.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 357kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 436M/436M [02:32<00:00, 2.86MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model = LanguageModelingModel(\n",
    "    \"bert\", \"bert-base-cased\", args=model_args, use_cuda=use_cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6438 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1238 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 1/6438 [00:04<8:06:24,  4.53s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "  8%|▊         | 537/6438 [00:04<00:37, 158.49it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 6438/6438 [00:05<00:00, 1172.55it/s]\n",
      "100%|██████████| 9072/9072 [00:00<00:00, 308675.41it/s]\n",
      "Epoch 1 of 1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Fine tune.\n",
    "model.train_model(train_file, eval_file=test_file)\n",
    "\n",
    "result = model.eval_model(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch / Huggingface code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def calculate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()  # we take the embedding of the [CLS] token\n",
    "    return embeddings\n",
    "\n",
    "# Calculate embeddings for each text\n",
    "df['embeddings'] = df['metadata_en_processed'].apply(calculate_embedding)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(np.vstack(df['embeddings']))\n",
    "\n",
    "# Find top 5 most similar texts for each text\n",
    "df['top_5_similar'] = [list(df.iloc[np.argsort(-row)][1:6].index) for row in similarity_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
