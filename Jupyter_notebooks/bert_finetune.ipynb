{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimpleparadox\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\rsaha/.netrc\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = json.load(open('D:\\similarity-engine\\wandb_config.json'))\n",
    "wandb.login(key=key['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# data = pd.read_parquet(\"../data/Processed_records.parquet\")\n",
    "# data = data.dropna()\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv(\"df_training_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into training and test sets.\n",
    "train_set = df.sample(frac=0.9, random_state=42)  # Fixing the seed to 42 to reproducibility.\n",
    "test_set = df.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe from simpletransformers. To fine-tune a language model, each sample should be a row in a text file.\n",
    "# # Store the 'metadata_en_processed' column in a text file.\n",
    "# with open('../data/simpletransformer_lm_train.txt', 'w') as f:\n",
    "#     for item in train_set['metadata_en_processed']:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# # Store the test set in a text file.\n",
    "# with open('../data/simpletransformer_lm_test.txt', 'w') as f:\n",
    "#     for item in test_set['metadata_en_processed']:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpletransformers code for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.num_train_epochs = 20\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.wandb_project = \"geo.ca\"\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.train_batch_size = 8\n",
    "model_args.eval_batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model arguments.\n",
    "# print(json.dumps(model_args.__dict__, indent=2))\n",
    "# print(len(model_args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the language modelling train file.\n",
    "train_file = \"../data/simpletransformer_lm_train.txt\"\n",
    "test_file = \"../data/simpletransformer_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model = LanguageModelingModel(\n",
    "    \"bert\", \"bert-base-uncased\", args=model_args, use_cuda=use_cuda\n",
    ")\n",
    "print(\"Running on GPU: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6438/6438 [00:24<00:00, 260.74it/s]\n",
      "100%|██████████| 8317/8317 [00:00<00:00, 105660.18it/s]\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\similarity-engine\\Jupyter_notebooks\\wandb\\run-20230727_145354-e9dqz31g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simpleparadox/geo.ca/runs/e9dqz31g' target=\"_blank\">true-sky-1</a></strong> to <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simpleparadox/geo.ca/runs/e9dqz31g' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca/runs/e9dqz31g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20:   0%|          | 0/20 [00:26<?, ?it/s]d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epochs 0/20. Running Loss:    3.6431: 100%|██████████| 1040/1040 [02:10<00:00,  7.96it/s]\n",
      "100%|██████████| 715/715 [00:14<00:00, 48.93it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 58244.38it/s]\n",
      "Epoch 2 of 20:   5%|▌         | 1/20 [03:12<1:00:49, 192.07s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:12<00:00, 57.34it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 58257.50it/s]\n",
      "Epochs 1/20. Running Loss:    1.3844: 100%|██████████| 1040/1040 [02:50<00:00,  6.09it/s]\n",
      "100%|██████████| 715/715 [00:16<00:00, 42.91it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 55018.61it/s]\n",
      "Epochs 2/20. Running Loss:    1.9168: 100%|██████████| 1040/1040 [02:02<00:00,  8.48it/s]\n",
      "100%|██████████| 715/715 [00:16<00:00, 42.98it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 55016.41it/s]\n",
      "Epoch 4 of 20:  15%|█▌        | 3/20 [09:16<51:16, 180.94s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:17<00:00, 40.04it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 70738.48it/s]\n",
      "Epochs 3/20. Running Loss:    1.6927: 100%|██████████| 1040/1040 [02:53<00:00,  5.98it/s]\n",
      "100%|██████████| 715/715 [00:16<00:00, 42.30it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 52120.23it/s]\n",
      "Epochs 4/20. Running Loss:    1.1854: 100%|██████████| 1040/1040 [02:07<00:00,  8.15it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 56.57it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 75919.88it/s]\n",
      "Epoch 6 of 20:  25%|██▌       | 5/20 [15:27<45:16, 181.10s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:14<00:00, 49.38it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 58264.07it/s]\n",
      "Epochs 5/20. Running Loss:    1.1004: 100%|██████████| 1040/1040 [02:49<00:00,  6.15it/s]\n",
      "100%|██████████| 715/715 [00:18<00:00, 38.74it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 59676.53it/s]\n",
      "Epochs 6/20. Running Loss:    0.9681: 100%|██████████| 1040/1040 [02:04<00:00,  8.34it/s]\n",
      "100%|██████████| 715/715 [00:21<00:00, 33.74it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 53483.83it/s]\n",
      "Epoch 8 of 20:  35%|███▌      | 7/20 [21:47<39:55, 184.25s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:14<00:00, 49.12it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 76182.81it/s]\n",
      "Epochs 7/20. Running Loss:    0.7212: 100%|██████████| 1040/1040 [02:47<00:00,  6.19it/s]\n",
      "100%|██████████| 715/715 [00:13<00:00, 52.26it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 82520.23it/s]\n",
      "Epochs 8/20. Running Loss:    0.8583: 100%|██████████| 1040/1040 [02:01<00:00,  8.54it/s]\n",
      "100%|██████████| 715/715 [00:13<00:00, 51.43it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 58069.36it/s]\n",
      "Epoch 10 of 20:  45%|████▌     | 9/20 [27:37<32:31, 177.42s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:13<00:00, 52.01it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 58254.22it/s]\n",
      "Epochs 9/20. Running Loss:    1.4251: 100%|██████████| 1040/1040 [02:36<00:00,  6.66it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 58.39it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 57914.85it/s]\n",
      "Epochs 10/20. Running Loss:    0.4013: 100%|██████████| 1040/1040 [02:07<00:00,  8.17it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 58.14it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 55013.49it/s]\n",
      "Epoch 12 of 20:  55%|█████▌    | 11/20 [33:24<26:07, 174.16s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.11it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 51732.95it/s]\n",
      "Epochs 11/20. Running Loss:    1.6115: 100%|██████████| 1040/1040 [02:39<00:00,  6.54it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.12it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 55021.54it/s]\n",
      "Epochs 12/20. Running Loss:    0.8631: 100%|██████████| 1040/1040 [02:06<00:00,  8.21it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 57.05it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 58445.22it/s]\n",
      "Epoch 14 of 20:  65%|██████▌   | 13/20 [39:09<20:03, 171.97s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:12<00:00, 57.68it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 62065.10it/s]\n",
      "Epochs 13/20. Running Loss:    0.6045: 100%|██████████| 1040/1040 [02:38<00:00,  6.56it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 58.46it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 58232.08it/s]\n",
      "Epochs 14/20. Running Loss:    0.4294: 100%|██████████| 1040/1040 [02:06<00:00,  8.25it/s]\n",
      "100%|██████████| 715/715 [00:13<00:00, 54.96it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 52111.69it/s]\n",
      "Epoch 16 of 20:  75%|███████▌  | 15/20 [44:58<14:19, 171.86s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:12<00:00, 58.19it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 57653.26it/s]\n",
      "Epochs 15/20. Running Loss:    1.5225: 100%|██████████| 1040/1040 [02:38<00:00,  6.58it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.30it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 61290.61it/s]\n",
      "Epochs 16/20. Running Loss:    0.8968: 100%|██████████| 1040/1040 [02:07<00:00,  8.18it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.40it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 66028.83it/s]\n",
      "Epoch 18 of 20:  85%|████████▌ | 17/20 [50:37<08:27, 169.10s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:16<00:00, 42.06it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 55023.00it/s]\n",
      "Epochs 17/20. Running Loss:    0.6941: 100%|██████████| 1040/1040 [02:41<00:00,  6.42it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.23it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 49515.50it/s]\n",
      "Epochs 18/20. Running Loss:    1.0735: 100%|██████████| 1040/1040 [02:06<00:00,  8.25it/s]\n",
      "100%|██████████| 715/715 [00:12<00:00, 59.51it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 57964.37it/s]\n",
      "Epoch 20 of 20:  95%|█████████▌| 19/20 [56:18<02:48, 168.20s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 715/715 [00:12<00:00, 57.79it/s]\n",
      "\n",
      "100%|██████████| 986/986 [00:00<00:00, 61900.67it/s]\n",
      "Epochs 19/20. Running Loss:    1.1254: 100%|██████████| 1040/1040 [02:38<00:00,  6.56it/s]\n",
      "100%|██████████| 715/715 [00:13<00:00, 54.57it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 61900.67it/s]\n",
      "Epoch 20 of 20: 100%|██████████| 20/20 [59:24<00:00, 178.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20800,\n",
       " {'global_step': [1040,\n",
       "   2000,\n",
       "   2080,\n",
       "   3120,\n",
       "   4000,\n",
       "   4160,\n",
       "   5200,\n",
       "   6000,\n",
       "   6240,\n",
       "   7280,\n",
       "   8000,\n",
       "   8320,\n",
       "   9360,\n",
       "   10000,\n",
       "   10400,\n",
       "   11440,\n",
       "   12000,\n",
       "   12480,\n",
       "   13520,\n",
       "   14000,\n",
       "   14560,\n",
       "   15600,\n",
       "   16000,\n",
       "   16640,\n",
       "   17680,\n",
       "   18000,\n",
       "   18720,\n",
       "   19760,\n",
       "   20000,\n",
       "   20800],\n",
       "  'perplexity': [tensor(10.1535),\n",
       "   tensor(6.6638),\n",
       "   tensor(6.4761),\n",
       "   tensor(5.0540),\n",
       "   tensor(4.8265),\n",
       "   tensor(4.5530),\n",
       "   tensor(4.3890),\n",
       "   tensor(4.1988),\n",
       "   tensor(3.9632),\n",
       "   tensor(3.8996),\n",
       "   tensor(3.7567),\n",
       "   tensor(3.8552),\n",
       "   tensor(3.4870),\n",
       "   tensor(3.5434),\n",
       "   tensor(3.3936),\n",
       "   tensor(3.3135),\n",
       "   tensor(3.3898),\n",
       "   tensor(3.3866),\n",
       "   tensor(3.2064),\n",
       "   tensor(3.2915),\n",
       "   tensor(3.1971),\n",
       "   tensor(3.0039),\n",
       "   tensor(3.1369),\n",
       "   tensor(3.1712),\n",
       "   tensor(3.0773),\n",
       "   tensor(3.1156),\n",
       "   tensor(3.1087),\n",
       "   tensor(3.0493),\n",
       "   tensor(3.0594),\n",
       "   tensor(3.0970)],\n",
       "  'eval_loss': [2.317821430342813,\n",
       "   1.8966902898203941,\n",
       "   1.8681242404685867,\n",
       "   1.6201778619039444,\n",
       "   1.5741204175377084,\n",
       "   1.515797303824295,\n",
       "   1.4790976129832767,\n",
       "   1.4347990346291373,\n",
       "   1.3770526125660962,\n",
       "   1.360867460889201,\n",
       "   1.3235387186249417,\n",
       "   1.3494350334897922,\n",
       "   1.2490480828189081,\n",
       "   1.2650777046238222,\n",
       "   1.2218951277675167,\n",
       "   1.1979982419119728,\n",
       "   1.2207740322656688,\n",
       "   1.2198178692675767,\n",
       "   1.165160960307525,\n",
       "   1.1913419877809863,\n",
       "   1.1622375480590328,\n",
       "   1.0999276208941968,\n",
       "   1.1432400835685492,\n",
       "   1.1541102650460802,\n",
       "   1.124058980464695,\n",
       "   1.1364348978405037,\n",
       "   1.1342053128586662,\n",
       "   1.1149129885879736,\n",
       "   1.1182170995180645,\n",
       "   1.1304263961651633],\n",
       "  'train_loss': [3.6431097984313965,\n",
       "   1.9003181457519531,\n",
       "   1.384377121925354,\n",
       "   1.9167637825012207,\n",
       "   1.8099435567855835,\n",
       "   1.692733645439148,\n",
       "   1.185407042503357,\n",
       "   1.1099059581756592,\n",
       "   1.1003682613372803,\n",
       "   0.9680838584899902,\n",
       "   0.7264732122421265,\n",
       "   0.7212415337562561,\n",
       "   0.8582918047904968,\n",
       "   1.0283864736557007,\n",
       "   1.4250761270523071,\n",
       "   0.4012640118598938,\n",
       "   1.1601369380950928,\n",
       "   1.6115479469299316,\n",
       "   0.8631418347358704,\n",
       "   0.9759775996208191,\n",
       "   0.6044872999191284,\n",
       "   0.4294298589229584,\n",
       "   0.8390750885009766,\n",
       "   1.522484302520752,\n",
       "   0.8968359231948853,\n",
       "   0.8209250569343567,\n",
       "   0.6940656304359436,\n",
       "   1.0735450983047485,\n",
       "   1.4613951444625854,\n",
       "   1.1254154443740845]})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune.\n",
    "model.train_model(train_file, eval_file=test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0003846917991286, 'perplexity': tensor(7.3919)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.eval_model(test_file)\n",
    "wandb.log(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch / Huggingface fine-tuning code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameters to consider when using huggingface.\n",
    "\n",
    "use fp16 to fine-tune.\n",
    "\n",
    "use the AdamW optimzer.\n",
    "\n",
    "scheduler: use linear schedule with warmup\n",
    "\n",
    "batch_size for everything: 8\n",
    "\n",
    "early stopping True\n",
    "\n",
    "early stopping patience: 3\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "\n",
    "learning_rate=4e-05\n",
    "\n",
    "max_grad_norm=1.0, \n",
    "\n",
    "max_seq_length=128\n",
    "\n",
    "warmup_ratio=0.06\n",
    "\n",
    "warmup_steps=0\n",
    "\n",
    "strip_accents=True\n",
    "\n",
    "handle_chinese_characters=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at D:\\similarity-engine\\Jupyter_notebooks\\outputs\\ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the BERT model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('D:\\\\similarity-engine\\\\Jupyter_notebooks\\\\outputs\\\\')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
