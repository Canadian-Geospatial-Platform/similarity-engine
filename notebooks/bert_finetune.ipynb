{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load packages.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimpleparadox\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\rsaha/.netrc\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = json.load(open('D:\\similarity-engine\\wandb_config.json'))\n",
    "wandb.login(key=key['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# data = pd.read_parquet(\"../data/Processed_records.parquet\")\n",
    "# data = data.dropna()\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv(\"df_training_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into training and test sets.\n",
    "train_set = df.sample(frac=0.9, random_state=42)  # Fixing the seed to 42 to reproducibility.\n",
    "test_set = df.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe from simpletransformers. To fine-tune a language model, each sample should be a row in a text file.\n",
    "# # Store the 'metadata_en_processed' column in a text file.\n",
    "# with open('../data/simpletransformer_lm_train.txt', 'w') as f:\n",
    "#     for item in train_set['metadata_en_processed']:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# # Store the test set in a text file.\n",
    "# with open('../data/simpletransformer_lm_test.txt', 'w') as f:\n",
    "#     for item in test_set['metadata_en_processed']:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpletransformers code for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.num_train_epochs = 20\n",
    "model_args.dataset_type = \"simple\"\n",
    "model_args.wandb_project = \"geo.ca\"\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.train_batch_size = 8\n",
    "model_args.eval_batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"adafactor_beta1\": null,\n",
      "  \"adafactor_clip_threshold\": 1.0,\n",
      "  \"adafactor_decay_rate\": -0.8,\n",
      "  \"adafactor_eps\": [\n",
      "    1e-30,\n",
      "    0.001\n",
      "  ],\n",
      "  \"adafactor_relative_step\": true,\n",
      "  \"adafactor_scale_parameter\": true,\n",
      "  \"adafactor_warmup_init\": true,\n",
      "  \"adam_betas\": [\n",
      "    0.9,\n",
      "    0.999\n",
      "  ],\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"best_model_dir\": \"outputs/best_model\",\n",
      "  \"cache_dir\": \"cache_dir/\",\n",
      "  \"config\": {},\n",
      "  \"cosine_schedule_num_cycles\": 0.5,\n",
      "  \"custom_layer_parameters\": [],\n",
      "  \"custom_parameter_groups\": [],\n",
      "  \"dataloader_num_workers\": 0,\n",
      "  \"do_lower_case\": false,\n",
      "  \"dynamic_quantize\": false,\n",
      "  \"early_stopping_consider_epochs\": false,\n",
      "  \"early_stopping_delta\": 0,\n",
      "  \"early_stopping_metric\": \"eval_loss\",\n",
      "  \"early_stopping_metric_minimize\": true,\n",
      "  \"early_stopping_patience\": 3,\n",
      "  \"encoding\": null,\n",
      "  \"eval_batch_size\": 8,\n",
      "  \"evaluate_during_training\": false,\n",
      "  \"evaluate_during_training_silent\": true,\n",
      "  \"evaluate_during_training_steps\": 2000,\n",
      "  \"evaluate_during_training_verbose\": false,\n",
      "  \"evaluate_each_epoch\": true,\n",
      "  \"fp16\": true,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"learning_rate\": 4e-05,\n",
      "  \"local_rank\": -1,\n",
      "  \"logging_steps\": 50,\n",
      "  \"loss_type\": null,\n",
      "  \"loss_args\": {},\n",
      "  \"manual_seed\": null,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"max_seq_length\": 128,\n",
      "  \"model_name\": null,\n",
      "  \"model_type\": null,\n",
      "  \"multiprocessing_chunksize\": -1,\n",
      "  \"n_gpu\": 1,\n",
      "  \"no_cache\": false,\n",
      "  \"no_save\": false,\n",
      "  \"not_saved_args\": [],\n",
      "  \"num_train_epochs\": 20,\n",
      "  \"optimizer\": \"AdamW\",\n",
      "  \"output_dir\": \"outputs/\",\n",
      "  \"overwrite_output_dir\": true,\n",
      "  \"polynomial_decay_schedule_lr_end\": 1e-07,\n",
      "  \"polynomial_decay_schedule_power\": 1.0,\n",
      "  \"process_count\": 61,\n",
      "  \"quantized_model\": false,\n",
      "  \"reprocess_input_data\": true,\n",
      "  \"save_best_model\": true,\n",
      "  \"save_eval_checkpoints\": true,\n",
      "  \"save_model_every_epoch\": true,\n",
      "  \"save_optimizer_and_scheduler\": true,\n",
      "  \"save_steps\": 2000,\n",
      "  \"scheduler\": \"linear_schedule_with_warmup\",\n",
      "  \"silent\": false,\n",
      "  \"skip_special_tokens\": true,\n",
      "  \"tensorboard_dir\": null,\n",
      "  \"thread_count\": null,\n",
      "  \"tokenizer_name\": null,\n",
      "  \"tokenizer_type\": null,\n",
      "  \"train_batch_size\": 8,\n",
      "  \"train_custom_parameters_only\": false,\n",
      "  \"use_cached_eval_features\": false,\n",
      "  \"use_early_stopping\": false,\n",
      "  \"use_hf_datasets\": false,\n",
      "  \"use_multiprocessing\": true,\n",
      "  \"use_multiprocessing_for_evaluation\": true,\n",
      "  \"wandb_kwargs\": {},\n",
      "  \"wandb_project\": \"geo.ca\",\n",
      "  \"warmup_ratio\": 0.06,\n",
      "  \"warmup_steps\": 0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"model_class\": \"LanguageModelingModel\",\n",
      "  \"block_size\": -1,\n",
      "  \"config_name\": null,\n",
      "  \"dataset_class\": null,\n",
      "  \"dataset_type\": \"simple\",\n",
      "  \"discriminator_config\": {},\n",
      "  \"discriminator_loss_weight\": 50.0,\n",
      "  \"generator_config\": {},\n",
      "  \"max_steps\": -1,\n",
      "  \"min_frequency\": 2,\n",
      "  \"mlm\": true,\n",
      "  \"mlm_probability\": 0.15,\n",
      "  \"sliding_window\": false,\n",
      "  \"special_tokens\": [\n",
      "    \"<s>\",\n",
      "    \"<pad>\",\n",
      "    \"</s>\",\n",
      "    \"<unk>\",\n",
      "    \"<mask>\"\n",
      "  ],\n",
      "  \"stride\": 0.8,\n",
      "  \"tie_generator_and_discriminator_embeddings\": true,\n",
      "  \"vocab_size\": null,\n",
      "  \"clean_text\": true,\n",
      "  \"handle_chinese_chars\": true,\n",
      "  \"special_tokens_list\": [],\n",
      "  \"strip_accents\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the model arguments.\n",
    "print(json.dumps(model_args.__dict__, indent=2))\n",
    "# print(len(model_args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the language modelling train file.\n",
    "train_file = \"../data/simpletransformer_lm_train.txt\"\n",
    "test_file = \"../data/simpletransformer_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model = LanguageModelingModel(\n",
    "    \"bert\", \"bert-base-uncased\", args=model_args, use_cuda=use_cuda\n",
    ")\n",
    "print(\"Running on GPU: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6438/6438 [00:19<00:00, 331.67it/s]\n",
      "100%|██████████| 8317/8317 [00:00<00:00, 61879.64it/s]\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\similarity-engine\\notebooks\\wandb\\run-20230803_110604-mzr4vcbm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simpleparadox/geo.ca/runs/mzr4vcbm' target=\"_blank\">helpful-moon-6</a></strong> to <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simpleparadox/geo.ca/runs/mzr4vcbm' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca/runs/mzr4vcbm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20:   0%|          | 0/20 [00:10<?, ?it/s]d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epochs 0/20. Running Loss:    3.0039:  51%|█████     | 526/1040 [01:05<01:03,  8.04it/s]\n",
      "Epoch 1 of 20:   0%|          | 0/20 [01:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Fine tune.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_file)\u001b[39m#, eval_file=test_file)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\simpletransformers\\language_modeling\\language_modeling_model.py:464\u001b[0m, in \u001b[0;36mLanguageModelingModel.train_model\u001b[1;34m(self, train_file, output_dir, show_running_loss, args, eval_file, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_and_cache_examples(train_file, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[0;32m    462\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 464\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[0;32m    465\u001b[0m     train_dataset,\n\u001b[0;32m    466\u001b[0m     output_dir,\n\u001b[0;32m    467\u001b[0m     show_running_loss\u001b[39m=\u001b[39mshow_running_loss,\n\u001b[0;32m    468\u001b[0m     eval_file\u001b[39m=\u001b[39meval_file,\n\u001b[0;32m    469\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    471\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(output_dir, model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melectra\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\simpletransformers\\language_modeling\\language_modeling_model.py:870\u001b[0m, in \u001b[0;36mLanguageModelingModel.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_file, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m    866\u001b[0m         model\u001b[39m.\u001b[39mparameters(), args\u001b[39m.\u001b[39mmax_grad_norm\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    869\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m--> 870\u001b[0m     scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[0;32m    871\u001b[0m     scaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    872\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    372\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 374\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[0;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:290\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 290\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    161\u001b[0m         group,\n\u001b[0;32m    162\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m         state_steps,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[0;32m    174\u001b[0m         exp_avgs,\n\u001b[0;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    177\u001b[0m         state_steps,\n\u001b[0;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 321\u001b[0m func(\n\u001b[0;32m    322\u001b[0m     params,\n\u001b[0;32m    323\u001b[0m     grads,\n\u001b[0;32m    324\u001b[0m     exp_avgs,\n\u001b[0;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    327\u001b[0m     state_steps,\n\u001b[0;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[0;32m    339\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\optim\\adamw.py:564\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    562\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(max_exp_avg_sq_sqrt, eps)\n\u001b[0;32m    563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 564\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    565\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    566\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine tune.\n",
    "model.train_model(train_file)#, eval_file=test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0003846917991286, 'perplexity': tensor(7.3919)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.eval_model(test_file)\n",
    "wandb.log(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch / Huggingface fine-tuning code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameters to consider when using huggingface.\n",
    "\n",
    "use fp16 to fine-tune.\n",
    "\n",
    "use the AdamW optimzer.\n",
    "\n",
    "scheduler: use linear schedule with warmup\n",
    "\n",
    "batch_size for everything: 8\n",
    "\n",
    "early stopping True\n",
    "\n",
    "early stopping patience: 3\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "\n",
    "learning_rate=4e-05\n",
    "\n",
    "max_grad_norm=1.0, \n",
    "\n",
    "max_seq_length=128\n",
    "\n",
    "warmup_ratio=0.06\n",
    "\n",
    "warmup_steps=0\n",
    "\n",
    "strip_accents=True\n",
    "\n",
    "handle_chinese_characters=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at D:\\similarity-engine\\Jupyter_notebooks\\outputs\\ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the BERT model\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained('D:\\\\similarity-engine\\\\Jupyter_notebooks\\\\outputs\\\\')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:\\\\similarity-engine\\\\src\\\\\")\n",
    "from utils.custom_dataset import CustomDataset\n",
    "from models.custom_model import CustomModel\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mzr4vcbm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>▄█▃▃▄▄▂▁▁▄</td></tr><tr><td>global_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>lr</td><td>▁▂▃▃▄▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>3.89149</td></tr><tr><td>global_step</td><td>500</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-moon-6</strong> at: <a href='https://wandb.ai/simpleparadox/geo.ca/runs/mzr4vcbm' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca/runs/mzr4vcbm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230803_110604-mzr4vcbm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mzr4vcbm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\similarity-engine\\notebooks\\wandb\\run-20230803_114237-r5kc0qug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simpleparadox/geo.ca/runs/r5kc0qug' target=\"_blank\">rare-sun-7</a></strong> to <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simpleparadox/geo.ca' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simpleparadox/geo.ca/runs/r5kc0qug' target=\"_blank\">https://wandb.ai/simpleparadox/geo.ca/runs/r5kc0qug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='geo.ca')\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_name = 'bert-base-uncased'\n",
    "config.model_type = 'bert'\n",
    "config.batch_size = 8\n",
    "config.epochs = 3\n",
    "config.lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "config.device = device\n",
    "print(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\similarity-engine\\\\notebooks\\\\df_training_full.csv')\n",
    "\n",
    "# Divide the data into train and test set. \n",
    "train_set = df.sample(frac=0.9, random_state=42)  # Fixing the seed to 42 to reproducibility.\n",
    "test_set = df.drop(train_set.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epoch 1 of 3:   1%|          | 6/805 [00:29<1:06:31,  5.00s/it]\n",
      "Epoch 1 of 3:   0%|          | 0/3 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[39m=\u001b[39m CustomModel(config)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Train the model.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_set)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mmodel)\n",
      "File \u001b[1;32mD:\\similarity-engine\\src\\models\\custom_model.py:88\u001b[0m, in \u001b[0;36mCustomModel.train_model\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     85\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     86\u001b[0m \u001b[39m# current_loss = loss.item()\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# batch_iterator.set_description(f\"Running epoch {epoch + 1} of {self.args.epochs}, current loss: {current_loss}\")\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     89\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     90\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "model = CustomModel(config)\n",
    "\n",
    "# Train the model.\n",
    "model.train_model(train_set)\n",
    "print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpletransformers models.\n",
    "Fine tuning BERT using LanguageModellingModel and the setting the finetuned model to a RepresentationModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# First encode a sentence with the pretrained representation model.\n",
    "\n",
    "# E.g., BERT pretrained.\n",
    "# Can be a model from HuggingFace's transformers library.\n",
    "model_type = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "rep_model = RepresentationModel(\n",
    "        model_type=model_type,\n",
    "        model_name=model_name,\n",
    "        use_cuda=True\n",
    "    )\n",
    "\n",
    "sentences = ['hello world 1', 'hello world 2']\n",
    "\n",
    "word_vectors = rep_model.encode_sentences(sentences, combine_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 768)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape\n",
    "# Take the first embedding of each sentence as the CLS embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fine tune a model.\n",
    "\n",
    "# Initialize the LM Args.\n",
    "model_args = LanguageModelingArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.dataset_type = \"simple\"\n",
    "# model_args.wandb_project = \"geo.ca\"\n",
    "model_args.evaluate_during_training = False\n",
    "model_args.train_batch_size = 8\n",
    "model_args.eval_batch_size = 8\n",
    "model_args.config = {\"output_hidden_states\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data paths.\n",
    "train_file = \"../data/simpletransformer_lm_train.txt\"\n",
    "# test_file = \"../data/simpletransformer_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model = LanguageModelingModel(\n",
    "    \"bert\", \"bert-base-uncased\", args=model_args, use_cuda=use_cuda\n",
    ")\n",
    "print(\"Running on GPU: {}\".format(use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6438 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\multiprocessing\\pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[0;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_file)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\simpletransformers\\language_modeling\\language_modeling_model.py:460\u001b[0m, in \u001b[0;36mLanguageModelingModel.train_model\u001b[1;34m(self, train_file, output_dir, show_running_loss, args, eval_file, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOutput directory (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) already exists and is not empty.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Set args.overwrite_output_dir = True to overcome.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(output_dir)\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_model_to_device()\n\u001b[1;32m--> 460\u001b[0m train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_and_cache_examples(train_file, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    462\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    464\u001b[0m global_step, training_details \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[0;32m    465\u001b[0m     train_dataset,\n\u001b[0;32m    466\u001b[0m     output_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    471\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\simpletransformers\\language_modeling\\language_modeling_model.py:1393\u001b[0m, in \u001b[0;36mLanguageModelingModel.load_and_cache_examples\u001b[1;34m(self, file_path, evaluate, no_cache, verbose, silent)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmax_seq_length \u001b[39m=\u001b[39m (\n\u001b[0;32m   1380\u001b[0m         \u001b[39m509\u001b[39m\n\u001b[0;32m   1381\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mbool\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m510\u001b[39m\n\u001b[0;32m   1385\u001b[0m     )\n\u001b[0;32m   1386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mblock_size \u001b[39m=\u001b[39m (\n\u001b[0;32m   1387\u001b[0m         \u001b[39m509\u001b[39m\n\u001b[0;32m   1388\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mbool\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m510\u001b[39m\n\u001b[0;32m   1392\u001b[0m     )\n\u001b[1;32m-> 1393\u001b[0m \u001b[39mreturn\u001b[39;00m SimpleDataset(\n\u001b[0;32m   1394\u001b[0m     tokenizer,\n\u001b[0;32m   1395\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[0;32m   1396\u001b[0m     file_path,\n\u001b[0;32m   1397\u001b[0m     mode,\n\u001b[0;32m   1398\u001b[0m     args\u001b[39m.\u001b[39;49mblock_size,\n\u001b[0;32m   1399\u001b[0m     special_tokens_count,\n\u001b[0;32m   1400\u001b[0m     sliding_window\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msliding_window,\n\u001b[0;32m   1401\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\simpletransformers\\language_modeling\\language_modeling_utils.py:179\u001b[0m, in \u001b[0;36mSimpleDataset.__init__\u001b[1;34m(self, tokenizer, args, file_path, mode, block_size, special_tokens_count, sliding_window)\u001b[0m\n\u001b[0;32m    176\u001b[0m         chunksize \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mmultiprocessing_chunksize\n\u001b[0;32m    178\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(args\u001b[39m.\u001b[39mprocess_count) \u001b[39mas\u001b[39;00m p:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m    180\u001b[0m             tqdm(\n\u001b[0;32m    181\u001b[0m                 p\u001b[39m.\u001b[39;49mimap(encode, lines, chunksize\u001b[39m=\u001b[39;49mchunksize),\n\u001b[0;32m    182\u001b[0m                 total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(lines),\n\u001b[0;32m    183\u001b[0m                 \u001b[39m# disable=silent,\u001b[39;49;00m\n\u001b[0;32m    184\u001b[0m             )\n\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples \u001b[39m=\u001b[39m [encode(line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\multiprocessing\\pool.py:420\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    412\u001b[0m result \u001b[39m=\u001b[39m IMapIterator(\u001b[39mself\u001b[39m)\n\u001b[0;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_taskqueue\u001b[39m.\u001b[39mput(\n\u001b[0;32m    414\u001b[0m     (\n\u001b[0;32m    415\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_guarded_task_generation(result\u001b[39m.\u001b[39m_job,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m         result\u001b[39m.\u001b[39m_set_length\n\u001b[0;32m    419\u001b[0m     ))\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m (item \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m result \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m chunk)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\multiprocessing\\pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\similarity_engine\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<simpletransformers.language_modeling.language_modeling_model.LanguageModelingModel at 0x2b0928cd8e0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bert'\n",
    "model_name = 'bert-base-uncased'\n",
    "rep_model1 = RepresentationModel(\n",
    "        model_type=model_type,\n",
    "        model_name=model_name,\n",
    "        use_cuda=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "sentences = ['hello world 1', 'hello world 2']\n",
    "rep_model1.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\similarity_engine\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = tokenizer.encode(['hello world!'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Encode the sentences.\n",
    "encodings = tokenizer.encode_plus(\n",
    "    text=['hello world1', 'hello world2'],  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = 512,  # maximum length of a sentence\n",
    "    padding='max_length',  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "encodings.to('cuda:0')\n",
    "print(encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.model(**encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -6.3007,  -6.2235,  -6.2208,  ...,  -5.6089,  -5.4883,  -3.4899],\n",
       "         [-11.8073, -12.0595, -11.9877,  ...,  -9.2797, -10.1229,  -6.2914],\n",
       "         [-10.2800, -10.1213, -10.5442,  ...,  -8.4385, -10.0934,  -2.0034],\n",
       "         ...,\n",
       "         [ -5.0119,  -4.9243,  -5.0544,  ...,  -4.4180,  -5.7131,   0.1921],\n",
       "         [ -5.1042,  -5.0389,  -5.1111,  ...,  -4.5710,  -5.5553,  -0.2378],\n",
       "         [ -6.5133,  -6.4716,  -6.5902,  ...,  -5.8658,  -6.1577,  -1.3167]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_list):\n",
    "    # Tokenize the text with the provided tokenizer\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        text_list,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "def batch_iterable(iterable, batch_size=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield iterable[ndx : min(ndx + batch_size, l)]\n",
    "\n",
    "def encode_sentences(text_list, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates list of contextual word or sentence embeddings using the model passed to class constructor\n",
    "    :param text_list: list of text sentences\n",
    "    :param combine_strategy: strategy for combining word vectors, supported values: None, \"mean\", \"concat\",\n",
    "    or an int value to select a specific embedding (e.g. 0 for [CLS] or -1 for the last one)\n",
    "    :param batch_size\n",
    "    :return: list of lists of sentence embeddings (if `combine_strategy=None`) OR list of sentence\n",
    "    embeddings (if `combine_strategy!=None`)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # model.to(device)\n",
    "    # model.eval()\n",
    "    embeddings = list()\n",
    "    batches = batch_iterable(text_list, batch_size=32)\n",
    "    for batch in batches:\n",
    "        encoded = tokenize(batch)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            token_vectors = model(\n",
    "                input_ids=encoded[\"input_ids\"].to(device),\n",
    "                attention_mask=encoded[\"attention_mask\"].to(device),\n",
    "                token_type_ids=encoded[\"token_type_ids\"].to(device),\n",
    "            )\n",
    "            # else:\n",
    "            #     token_vectors = self.model(\n",
    "            #         input_ids=encoded[\"input_ids\"].to(self.device),\n",
    "            #         attention_mask=encoded[\"attention_mask\"].to(self.device),\n",
    "            #     )\n",
    "            return token_vectors\n",
    "        embeddings.append(token_vectors.detach().numpy())\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = encode_sentences(['hello world1'], model.model, 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -6.4943,  -6.4595,  -6.3881,  ...,  -5.6625,  -5.5103,  -4.1114],\n",
       "         [ -9.5738,  -9.3839,  -9.1378,  ...,  -9.6869,  -7.1647,  -7.2835],\n",
       "         [ -9.0334,  -9.3464,  -9.1769,  ...,  -9.1942,  -7.9128,  -4.7101],\n",
       "         [ -9.0591,  -9.2856,  -9.5194,  ...,  -9.2929,  -8.7199,  -5.6519],\n",
       "         [-12.1725, -12.0755, -11.7472,  ...,  -8.9967,  -9.7611,  -8.5034]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
